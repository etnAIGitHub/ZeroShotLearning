{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import path\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pytorch.utils import Denormalize_tensor, Show_figure_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"COCO-DoubleCNNContextClassification-RN50+RN18-SGD\"\n",
    "epochs = 120\n",
    "batch_size = 48\n",
    "num_batch_train = 200\n",
    "num_batch_val = 80\n",
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_path = path.join(\"parameters\", exp_name)\n",
    "plots_path = path.join(\"plots\", exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from os import path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from albumentations import Compose, Normalize, VerticalFlip, HorizontalFlip, Rotate, Resize, LongestMaxSize, PadIfNeeded, BboxParams\n",
    "from albumentations.pytorch import ToTensor, ToTensorV2\n",
    "\n",
    "from Pytorch.utils import Normalize_bbox_to_0_1, Convert_bbox_from_TLWH_to_TLBR, clamp\n",
    "\n",
    "'''\n",
    "CODING CONVENTIONS\n",
    "\n",
    "- All names use underscore casing\n",
    "- L_* for list variable names\n",
    "- D_* for dictionary variable names\n",
    "- T_* for tensor variable names\n",
    "- M_* for Pytorch models\n",
    "- DL_* for Data Loader\n",
    "- DSET_* for Dataset\n",
    "- Class names are in upper case\n",
    "- Function names start with a capital letter\n",
    "'''\n",
    "\n",
    "AlbuToNormalizedTensor_transform = Compose(\n",
    "    [\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def AlbuToNormalizedTensor(image):\n",
    "    return AlbuToNormalizedTensor_transform(image=image)['image']\n",
    "\n",
    "class CONTEXT_DATASET(Dataset):\n",
    "    def __init__(self,\n",
    "                root_dirpath,\n",
    "                images_dirpath,\n",
    "                annotations_path,\n",
    "                image_id_key=\"image_id\",\n",
    "                category_id_key=\"category_id\",\n",
    "                F_image_id_to_relative_path=None,\n",
    "                augmentation=True,\n",
    "                desired_size=224,\n",
    "                batch_size=12,\n",
    "                num_batch=1):\n",
    "\n",
    "        self.root_dirpath = root_dirpath\n",
    "        self.images_dirpath = images_dirpath\n",
    "        self.annotations_path = annotations_path\n",
    "        self.image_id_key=image_id_key\n",
    "        self.category_id_key=category_id_key\n",
    "        self.F_image_id_to_relative_path = F_image_id_to_relative_path\n",
    "        self.augmentation = augmentation\n",
    "        self.desired_size = desired_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batch = num_batch\n",
    "\n",
    "        with open(self.annotations_path) as f:\n",
    "            self.D_instances = json.load(f)\n",
    "            self.L_annotations = self.D_instances['annotations']\n",
    "            self.L_categories = self.D_instances['categories']\n",
    "\n",
    "        # list of real class indices in COCO dataset\n",
    "        self.L_real_class_idx = [x['id'] for x in self.L_categories]\n",
    "        # zero-based class indices of COCO dataset (0-79)\n",
    "        self.L_class_idx = range(len(self.L_real_class_idx))\n",
    "        # dict to map real class indices to zero-based indices\n",
    "        self.D_real_class_idx_to_class_idx = { real_class_idx : class_idx for class_idx, real_class_idx in enumerate(self.L_real_class_idx)}\n",
    "\n",
    "        # dict to map zero based indices to the relative annotations\n",
    "        self.D_class_to_annotations = self.Create_class_to_annotations_dict()\n",
    "\n",
    "        # filter zero-based class indices which have no annotations\n",
    "        self.L_not_empty_class_idx = [class_idx for class_idx in self.L_class_idx if len(self.D_class_to_annotations[class_idx])>0]\n",
    "\n",
    "        bbox_params = BboxParams(format='coco', label_fields=[self.category_id_key])\n",
    "        self.augmentation_transform = Compose(\n",
    "            [\n",
    "                VerticalFlip(p=0.5),\n",
    "                HorizontalFlip(p=0.5),\n",
    "                LongestMaxSize(max_size=600),\n",
    "                PadIfNeeded(min_height=710, min_width=710, border_mode=cv2.BORDER_CONSTANT),\n",
    "                Rotate(p=1.0, limit=12, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT),\n",
    "                #Resize(height=desired_size, width=desired_size),\n",
    "            ],\n",
    "            bbox_params=bbox_params)\n",
    "        \n",
    "        self.last_resize_transform = Resize(height=desired_size, width=desired_size)\n",
    "\n",
    "        self.loading_transform = Compose(\n",
    "            [\n",
    "                LongestMaxSize(max_size=desired_size),\n",
    "                PadIfNeeded(min_height=desired_size, min_width=desired_size, border_mode=cv2.BORDER_CONSTANT),\n",
    "            ],\n",
    "            bbox_params=bbox_params)\n",
    "\n",
    "        self.transform = self.augmentation_transform if self.augmentation else self.loading_transform\n",
    "\n",
    "    def Create_class_to_annotations_dict(self):\n",
    "        '''\n",
    "        Create a dictionary of type (int : list) which map to each zero-based class index a list of\n",
    "        all the COCO annotations of a specific class using self.D_real_class_idx_to_class_idx as map\n",
    "        '''\n",
    "        # init empty dictionary with zero-based class indices\n",
    "        D_class_to_annotations = {idx : [] for idx in self.L_class_idx}\n",
    "\n",
    "        for annotation in self.L_annotations:\n",
    "            real_class_idx = annotation[self.category_id_key]\n",
    "            class_idx = self.D_real_class_idx_to_class_idx[real_class_idx]\n",
    "            D_class_to_annotations[class_idx].append(annotation)\n",
    "\n",
    "        return D_class_to_annotations\n",
    "\n",
    "    def Get_num_classes(self):\n",
    "        return len(self.L_categories)\n",
    "\n",
    "    def Get_class_idx_description(self, class_idx):\n",
    "        return self.L_categories[class_idx]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            # sample a random annotation from a random class\n",
    "            item_class_idx = np.random.choice(self.L_not_empty_class_idx)\n",
    "            D_item_annotation = np.random.choice(self.D_class_to_annotations[item_class_idx])\n",
    "\n",
    "            item_image_id = D_item_annotation[self.image_id_key]\n",
    "\n",
    "            if(self.F_image_id_to_relative_path):\n",
    "                image_relative_path = self.F_image_id_to_relative_path(item_image_id)\n",
    "            else:\n",
    "                image_relative_path = item_image_id\n",
    "            item_PIL_image = Image.open(path.join(self.images_dirpath, image_relative_path)).convert('RGB')\n",
    "            image_width, image_height = item_PIL_image.size\n",
    "\n",
    "            item_bbox = D_item_annotation['bbox']\n",
    "            item_bbox[0] = clamp(item_bbox[0], 0, image_width)\n",
    "            item_bbox[1] = clamp(item_bbox[1], 0, image_height)\n",
    "            item_bbox[2] = clamp(item_bbox[2], 0, image_width-item_bbox[0])\n",
    "            item_bbox[3] = clamp(item_bbox[3], 0, image_height-item_bbox[1])\n",
    "\n",
    "            D_item_albumentation = {'image': np.array(item_PIL_image), 'bboxes': [item_bbox], self.category_id_key: [item_class_idx]}\n",
    "\n",
    "            D_item_transformed = self.transform(**D_item_albumentation)\n",
    "            if(len(D_item_transformed['bboxes'])==0):\n",
    "                return self[0]\n",
    "\n",
    "            # D_item_transformed['bboxes'] has got the only bbox we are passing in D_item_albumentation\n",
    "            L_bbox_TLBR = Convert_bbox_from_TLWH_to_TLBR(list(D_item_transformed['bboxes'][0]))\n",
    "            L_bbox_normalized = Normalize_bbox_to_0_1(L_bbox_TLBR, (710, 710))\n",
    "            T_bbox_normalized = torch.Tensor(L_bbox_normalized)\n",
    "            \n",
    "            augmented_image = D_item_transformed[\"image\"]\n",
    "            L_bbox_TLBR = [int(x) for x in L_bbox_TLBR]\n",
    "            cropped_image = augmented_image[L_bbox_TLBR[1]:L_bbox_TLBR[3], L_bbox_TLBR[0]:L_bbox_TLBR[2]]\n",
    "            cropped_image = self.loading_transform(**{'image': cropped_image, 'bboxes': [], self.category_id_key: []})['image']\n",
    "            augmented_image_resize=self.last_resize_transform(image=augmented_image)[\"image\"]\n",
    "            return (AlbuToNormalizedTensor(cropped_image), AlbuToNormalizedTensor(augmented_image_resize)), item_class_idx\n",
    "        except:\n",
    "            return self[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batch*self.batch_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET_training = CONTEXT_DATASET(\n",
    "    root_dirpath = '../../../Datasets/COCO',\n",
    "    images_dirpath = '../../../Datasets/COCO/train2017',\n",
    "    annotations_path = '../../../Datasets/COCO/annotations/instances_train2017.json',\n",
    "    F_image_id_to_relative_path = lambda image_id: \"{:012d}.jpg\".format(image_id),\n",
    "    augmentation = True,\n",
    "    batch_size = batch_size,\n",
    "    num_batch = num_batch_train,\n",
    "    desired_size=img_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSET_validation = CONTEXT_DATASET(\n",
    "    root_dirpath = '../../../Datasets/COCO',\n",
    "    images_dirpath = '../../../Datasets/COCO/val2017',\n",
    "    annotations_path = '../../../Datasets/COCO/annotations/instances_val2017.json',\n",
    "    F_image_id_to_relative_path = lambda image_id: \"{:012d}.jpg\".format(image_id),\n",
    "    augmentation = False,\n",
    "    batch_size = batch_size,\n",
    "    num_batch = num_batch_val,\n",
    "    desired_size=img_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalize_func = Denormalize_tensor(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(T_img, T_context), class_idx = DSET_training[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transforms.ToPILImage()(denormalize_func(T_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transforms.ToPILImage()(denormalize_func(T_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(T_img, T_context), class_idx = DSET_validation[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transforms.ToPILImage()(denormalize_func(T_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(transforms.ToPILImage()(denormalize_func(T_context)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_training = DataLoader(DSET_training, batch_size=batch_size, shuffle=False)\n",
    "DL_validation = DataLoader(DSET_validation, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class M_CONTEXT_CLASSIFIER(torch.nn.Module):\n",
    "    def __init__(self, num_classes, backbone=None):\n",
    "        super(M_CONTEXT_CLASSIFIER, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.M_context_cnn = resnet50(pretrained=True)\n",
    "        self.M_context_cnn.fc = torch.nn.Linear(in_features=self.M_context_cnn.fc.in_features, out_features=1024, bias=True)\n",
    "        \n",
    "        self.M_patch_cnn = resnet50(pretrained=True)\n",
    "        self.M_patch_cnn.fc = torch.nn.Linear(in_features=self.M_patch_cnn.fc.in_features, out_features=1024, bias=True)\n",
    "\n",
    "        self.fc = torch.nn.Linear(in_features=self.M_patch_cnn.fc.out_features+self.M_context_cnn.fc.out_features, out_features=self.num_classes, bias=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        T_image, T_context = inputs\n",
    "        T_extracted_features_patch = self.M_patch_cnn(T_image)\n",
    "        T_extracted_features_context = self.M_context_cnn(T_context)\n",
    "        T_features = torch.cat((T_extracted_features_patch, T_extracted_features_context), 1)\n",
    "        return self.fc(T_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_classifier = M_CONTEXT_CLASSIFIER(num_classes=DSET_training.Get_num_classes())\n",
    "M_classifier = M_classifier.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch_fn(batch, gt):\n",
    "    gt = gt.long()\n",
    "    return batch, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtrainer.trainer import Trainer, Mode\n",
    "from torchtrainer.callbacks.calculateaccuracycallback import CalculateTopNAccuracyCallback\n",
    "from torchtrainer.callbacks.calculatelosscallback import CalculateLossCallback\n",
    "from torchtrainer.callbacks.plotcallback import PlotCallback\n",
    "from torchtrainer.callbacks.saveparameterscallback import SaveParametersCallback\n",
    "from torchtrainer.callbacks.settqdmbardescription import SetTQDMBarDescription\n",
    "from torchtrainer.callbacks.lrbatchschedulercallback import LRBatchSchedulerCallBack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(M_classifier.parameters(), lr = 0.08, momentum = 0.1)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    device = 'cuda',\n",
    "    modes = [Mode.TRAIN, Mode.EVALUATE],\n",
    "    model = M_classifier,\n",
    "    data_loaders = {Mode.TRAIN : DL_training, Mode.EVALUATE : DL_validation},\n",
    "    epochs = epochs,\n",
    "    starting_epoch = 0,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    prepare_batch_fn = prepare_batch_fn,\n",
    "    callbacks = [\n",
    "        CalculateLossCallback(key='Loss'),\n",
    "        CalculateTopNAccuracyCallback(keys=('Top-1 accuracy',), topk=(1,)),\n",
    "        PlotCallback(plots_path, labels_map={Mode.TRAIN:\"Train\", Mode.EVALUATE:\"Val\"}, columns=['Loss', 'Top-1 accuracy']),\n",
    "        SetTQDMBarDescription(keys=['Loss', 'Top-1 accuracy']),\n",
    "        SaveParametersCallback(parameters_path),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(M_classifier.parameters(), lr=1e-7, momentum=0.0)\n",
    "lr_finder = LRFinder(M_classifier, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(DL_training, end_lr=2, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(M_classifier.parameters(), lr=1e-7, momentum=0.2)\n",
    "lr_finder = LRFinder(M_classifier, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(DL_training, end_lr=2, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(M_classifier.parameters(), lr=1e-7, momentum=0.6)\n",
    "lr_finder = LRFinder(M_classifier, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(DL_training, end_lr=2, num_iter=100)\n",
    "lr_finder.plot() # to inspect the loss-learning rate graph\n",
    "lr_finder.reset() # to reset the model and optimizer to their initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('miniconda3': virtualenv)",
   "language": "python",
   "name": "python37664bitminiconda3virtualenv13a769f9b7a9422e87f2d15a4ea6cf2c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
