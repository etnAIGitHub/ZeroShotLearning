{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from PIL import Image\n",
    "from os import path, listdir\n",
    "import re\n",
    "import json\n",
    "\n",
    "ffmpeg_binpath = r'C:/Program Files/ffmpeg-4.2.2-win64/bin/ffmpeg.exe'\n",
    "root_path = r\"D:/Sources/BenedettiniBBox/2_Points_of_Interest_Recognition/Points Of Interest Recognition/Monastero dei Benedettini/\"\n",
    "train_path = path.join(root_path, \"Training/\")\n",
    "test_path = path.join(root_path, \"Test/\")\n",
    "val_path = path.join(root_path, \"Validation/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataFrameTrainingClassesMap(train_path):   \n",
    "    class_names = listdir(train_path)\n",
    "    class_names.sort(key=lambda x: (lambda l : int(l[0])*100+int(l[1]))(re.split('\\.|\\_', x)))\n",
    "    classes_map_df = pd.DataFrame(list(zip(class_names, range(37))), columns=['class_name', 'class_label'])\n",
    "    classes_map_df.set_index('class_name', inplace=True)\n",
    "    return classes_map_df\n",
    "\n",
    "classes_map_df = createDataFrameTrainingClassesMap(train_path)\n",
    "classes_map_df.to_pickle('classes_map_df.pickle')\n",
    "\n",
    "#classes_map_df = pd.read_pickle('classes_map_df.pickle')\n",
    "\n",
    "classes_map_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_categories = {\"categories\": []}\n",
    "\n",
    "for row, item in classes_map_df.iterrows():\n",
    "    D_categories[\"categories\"].append({'id':int(item[0]), 'name':row})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "\n",
    "def createDataFrameTrainingClassFromCSV(class_path):\n",
    "    # handle both class_path ending with/without '/'\n",
    "    class_name = class_path.split('/')[-2] if class_path[-1] == '/' else class_path.split('/')[-1]\n",
    "    try:\n",
    "        df_even = pd.read_csv(path.join(class_path, class_name+'_1.csv'))\n",
    "        df_even = df_even.set_index(pd.Index(range(len(df_even)*2)[1::2]))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"file {}_1.csv not found\".format(class_name))\n",
    "    try:\n",
    "        df_odd = pd.read_csv(path.join(class_path, class_name+'_2.csv'))\n",
    "        df_odd = df_odd.set_index(pd.Index(range(len(df_odd)*2)[::2]))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"file {}_2.csv not found\".format(class_name))\n",
    "    df = df_even.append(df_odd)\n",
    "    df = df[df['region_count'] != 0]\n",
    "    df.sort_index(inplace=True)\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df[['#filename','region_count','region_shape_attributes']]\n",
    "    return df\n",
    "    \n",
    "def createDataFrameTrainingClassFromTxt(class_path):\n",
    "    list_df = []\n",
    "    class_name = class_path.split('/')[-1]\n",
    "    if not len(class_name): class_name = class_path.split('/')[-2]\n",
    "\n",
    "    annotations_file_name = [annotation for annotation in listdir(path.join(class_path, 'images')) if annotation.endswith('.txt')]\n",
    "    annotations_file_name.sort(key=lambda x: (lambda l : int(l[1]))(re.split('frame_000|\\.', x)))\n",
    "\n",
    "    for annotation_file_name in annotations_file_name:\n",
    "        try:\n",
    "            annotation_file_path = path.join(class_path, 'images', annotation_file_name)\n",
    "            filename = annotation_file_name[:-3]+'jpg'\n",
    "            img_path = path.join(class_path, 'images', filename)\n",
    "            im_width, im_height = Image.open(img_path).size\n",
    "            region_shape_attributes_list = [line.split() for line in open(annotation_file_path)]\n",
    "\n",
    "            for region_shape_attributes in region_shape_attributes_list:\n",
    "                _, center_box_w, center_box_h, box_w, box_h = list(map(float, region_shape_attributes))\n",
    "                region_shape_attributes = '{{\"name\":\"rect\",\"x\":{},\"y\":{},\"width\":{},\"height\":{}}}'.format(\n",
    "                    int(center_box_w*im_width - (box_w*im_width)//2),\n",
    "                    int(center_box_h*im_height - (box_h*im_height)//2),\n",
    "                    int(box_w*im_width),\n",
    "                    int(box_h*im_height)\n",
    "                )\n",
    "                region_count = len(region_shape_attributes_list)\n",
    "                list_df.append([filename, region_count, region_shape_attributes])\n",
    "        except FileNotFoundError as e:\n",
    "            pass\n",
    "        \n",
    "    return pd.DataFrame(list_df, columns=['#filename', 'region_count', 'region_shape_attributes'])\n",
    "\n",
    "def createTrainingDatasetJSONFromCSV(train_path, classes_map_df):\n",
    "    training_dataset_json = {\"annotations\": []}\n",
    "    \n",
    "    for class_name, (class_label,) in classes_map_df.iterrows():\n",
    "        class_path = path.join(train_path, class_name)\n",
    "        try:\n",
    "            df = createDataFrameTrainingClassFromCSV(class_path)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e, \", loading annotations from .txt files\", sep=\"\")\n",
    "            df = createDataFrameTrainingClassFromTxt(class_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        for idx, record in df.iterrows():\n",
    "            img_name, region_count, bb_dict_str = record\n",
    "            #convert string to dict\n",
    "            bb_dict = eval(bb_dict_str)\n",
    "            bbox = [bb_dict['x'], bb_dict['y'], bb_dict['width'], bb_dict['height']]\n",
    "\n",
    "            training_dataset_json[\"annotations\"].append({\n",
    "                \"path\": \"Training/{}/images/{}\".format(class_name, img_name),\n",
    "                \"bbox\": bbox,\n",
    "                \"class_id\": class_label\n",
    "            })\n",
    "            \n",
    "    return training_dataset_json\n",
    "\n",
    "def createTrainingDatasetJSONFromTxt(train_path, classes_map_df):\n",
    "    training_dataset_json = {\"annotations\": []}\n",
    "    \n",
    "    for class_name, (class_label,) in classes_map_df.iterrows():\n",
    "        class_path = path.join(train_path, class_name)\n",
    "        try:\n",
    "            df = createDataFrameTrainingClassFromTxt(class_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        for idx, record in df.iterrows():\n",
    "            img_name, region_count, bb_dict_str = record\n",
    "            #convert string to dict\n",
    "            bb_dict = eval(bb_dict_str)\n",
    "            bbox = [bb_dict['x'], bb_dict['y'], bb_dict['width'], bb_dict['height']]\n",
    "\n",
    "            training_dataset_json[\"annotations\"].append({\n",
    "                \"path\": \"Training/{}/images/{}\".format(class_name, img_name),\n",
    "                \"bbox\": bbox,\n",
    "                \"class_id\": class_label\n",
    "            })\n",
    "            \n",
    "    return training_dataset_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestDatasetJSONFromTxt(test_path, classes_map_df):\n",
    "    test_dataset_json = {\"annotations\": []}\n",
    "\n",
    "    L_folder_ID = list(range(100, 167))\n",
    "    S_unwanted_ID = {106, 121, 127, 128, 131, 133, 141} \n",
    "    L_folder_ID = [ID for ID in L_folder_ID if ID not in S_unwanted_ID]\n",
    "    \n",
    "    for folder_ID in L_folder_ID:\n",
    "        try:\n",
    "            annotation_filepath = path.join(test_path, \"bbox_annotations\", str(folder_ID)+\".txt\")\n",
    "            L_annotations = [line.split() for line in open(annotation_filepath)][1:]\n",
    "\n",
    "            for annotation in L_annotations:\n",
    "                try:\n",
    "                    filename = annotation[0]\n",
    "                    region_count = int(annotation[3])\n",
    "                    if region_count == 0: continue\n",
    "                    img_path = path.join(test_path, str(folder_ID), filename)\n",
    "                    im_width, im_height = Image.open(img_path).size\n",
    "                    bb_dict = literal_eval(annotation[5])\n",
    "                    bbox = [bb_dict['x'], bb_dict['y'], bb_dict['width'], bb_dict['height']]\n",
    "                    region_attributes = annotation[-1]\n",
    "                    class_name_start = literal_eval(region_attributes)['Label']\n",
    "                    class_label = classes_map_df[classes_map_df.index.str.startswith(class_name_start+'_')]['class_label'].item()\n",
    "\n",
    "                    test_dataset_json[\"annotations\"].append({\n",
    "                        \"path\": \"Test/{}/{}\".format(folder_ID, filename),\n",
    "                        \"bbox\": bbox,\n",
    "                        \"class_id\": class_label\n",
    "                    })\n",
    "\n",
    "                except FileNotFoundError as e:\n",
    "                    pass\n",
    "                \n",
    "        except FileNotFoundError as e:\n",
    "            print(\"file {} not found\".format(path.join(\"bbox_annotations\", str(folder_ID)+\".txt\")))\n",
    "            \n",
    "    return test_dataset_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createValidationDatasetJSONFromTxt(val_path, classes_map_df):\n",
    "    val_dataset_json = {\"annotations\": []}\n",
    "    \n",
    "    for annotation_filename in listdir(path.join(val_path, \"bbox_annotations\")):\n",
    "        annotation_filepath = path.join(val_path, \"bbox_annotations\", annotation_filename)\n",
    "\n",
    "        L_annotations = [line.replace('\"\"', '\"').split(',') for line in open(annotation_filepath)][1:]\n",
    "        \n",
    "        for annotation in L_annotations:\n",
    "            try:\n",
    "                filename = annotation[0].replace(\" \", \"_\")\n",
    "                region_count = int(annotation[3])\n",
    "                if region_count == 0: continue\n",
    "\n",
    "                str_dict = annotation[5]+','+annotation[6]+','+annotation[7]+','+annotation[8]+','+annotation[9]\n",
    "                bb_dict = literal_eval(str_dict[1:-1])\n",
    "                bbox = [bb_dict['x'], bb_dict['y'], bb_dict['width'], bb_dict['height']]\n",
    "                class_name_start = literal_eval(annotation[-1][1:-2])['Label']\n",
    "                class_label = classes_map_df[classes_map_df.index.str.startswith(class_name_start+'_')]['class_label'].item()\n",
    "\n",
    "                val_dataset_json[\"annotations\"].append({\n",
    "                    \"path\": \"Validation/images/{}\".format(filename),\n",
    "                    \"bbox\": bbox,\n",
    "                    \"class_id\": class_label\n",
    "                })\n",
    "            except ValueError as e:\n",
    "                print(class_name_start, 'label not recognized')\n",
    "                \n",
    "    return val_dataset_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "def imageBBFromJSONObject(root_path, sample_JSONObj):\n",
    "    # Create figure and axes\n",
    "    fig, ax = plt.subplots(1)\n",
    "    fig.set_size_inches(8, 5)\n",
    "\n",
    "    img = Image.open(path.join(root_path, sample_JSONObj['path']))\n",
    "    class_name = sample_JSONObj['path'].split('/')[0]\n",
    "    img_name = sample_JSONObj['path'].split('/')[-1]\n",
    "    ax.set_title(\"[{}]  [{}]  {}\".format(class_name, sample_JSONObj['class_id'], img_name))\n",
    "\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    x, y, widht, height = sample_JSONObj['bbox']\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((x, y), widht, height, linewidth=3, edgecolor='r', facecolor='none')\n",
    "\n",
    "    # Add the patch to the Axes\n",
    "    rect_patch = ax.add_patch(rect)\n",
    "    \n",
    "    \n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['animation.ffmpeg_path'] = ffmpeg_binpath\n",
    "\n",
    "def videoBB(root_path, dataset_json, num_sample=10, interval=100):\n",
    "    fig = plt.figure(figsize=(13,8))\n",
    "    ax = plt.gca()\n",
    "    ax.axis('off')\n",
    "\n",
    "    ax_img = ax.imshow(Image.new('RGB', (1280, 720), color = 'white'))\n",
    "    \n",
    "    len_training_dataset = len(dataset_json['annotations'])\n",
    "    samples_idx = np.arange(0, len_training_dataset, len_training_dataset//num_sample)[:num_sample]\n",
    "\n",
    "    def animate(frame):\n",
    "\n",
    "        sample_JSONObj = dataset_json['annotations'][samples_idx[frame]]\n",
    "        img = Image.open(path.join(root_path, sample_JSONObj['path']))\n",
    "        class_name = sample_JSONObj['path'].split('/')[0]\n",
    "        img_name = sample_JSONObj['path'].split('/')[-1]\n",
    "        ax.set_title(\"[{}]  [{}]  {}\".format(class_name, sample_JSONObj['class_id'], img_name))\n",
    "        ax_img.set_array(img)\n",
    "\n",
    "        [p.remove() for p in reversed(ax.patches)]\n",
    "\n",
    "        x, y, widht, height = sample_JSONObj['bbox']\n",
    "        rect = patches.Rectangle((x, y), widht, height, linewidth=3, edgecolor='r', facecolor='none')\n",
    "        rect_patch = ax.add_patch(rect)\n",
    "\n",
    "    anim = FuncAnimation(fig, animate, frames=num_sample, interval=interval)\n",
    "    video = anim.to_html5_video(embed_limit=10000)\n",
    "    html = display.HTML(video)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(html)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset_json = createTestDatasetJSONFromTxt(test_path, classes_map_df)\n",
    "test_dataset_json.update(D_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(test_dataset_json['dataset'][:3], indent=2))\n",
    "len(test_dataset_json['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_dataset_json = createTrainingDatasetJSONFromCSV(train_path, classes_map_df)\n",
    "training_dataset_json.update(D_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(training_dataset_json['dataset'][:3], indent=2))\n",
    "len(training_dataset_json['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_json = createValidationDatasetJSONFromTxt(val_path, classes_map_df)\n",
    "val_dataset_json.update(D_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(json.dumps(val_dataset_json['dataset'][:3], indent=2))\n",
    "len(val_dataset_json['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"test_dataset.json\", \"w\") as output_file:\n",
    "    json.dump(test_dataset_json, output_file)\n",
    "\n",
    "with open(\"train_dataset.json\", \"w\") as output_file:\n",
    "    json.dump(training_dataset_json, output_file)\n",
    "    \n",
    "with open(\"val_dataset.json\", \"w\") as output_file:\n",
    "    json.dump(val_dataset_json, output_file)\n",
    "    \n",
    "#with open('val_dataset.json') as input_file:\n",
    "#    val_dataset_json = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageBBFromJSONObject(root_path, sample_JSONObj = training_dataset_json['annotations'][12414])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoBB(root_path, training_dataset_json, num_sample=5, interval=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
